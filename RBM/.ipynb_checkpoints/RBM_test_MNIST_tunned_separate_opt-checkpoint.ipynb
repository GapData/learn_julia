{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how to control memory allocation\n",
    "##### (Of functions that are called lots of times and generate arrays everytime they are called)\n",
    "\n",
    "\n",
    "\n",
    "This notebook compares two versions of a function:  **```compute_grad```** and **```compute_grad_with_dot!```**.\n",
    "\n",
    "The idea was to have a type that has \"placeholders\" for the quantities that are computed inside ```compute_grad``` (sampling quantities, as well as V_hat, H_hat) to avoid allocating memory at every update of the parameters of the model (at every call to  ```compute_grad```).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: deprecated syntax \"typealias Partition Vector{Int}\" at /Users/macpro/.julia/v0.6/Combinatorics/src/youngdiagrams.jl:6.\n",
      "Use \"const Partition = Vector{Int}\" instead.\n",
      "\n",
      "WARNING: deprecated syntax \"typealias YoungDiagram Array{Int,2}\" at /Users/macpro/.julia/v0.6/Combinatorics/src/youngdiagrams.jl:7.\n",
      "Use \"const YoungDiagram = Array{Int,2}\" instead.\n",
      "\n",
      "WARNING: deprecated syntax \"typealias SkewDiagram Tuple{Partition,Partition}\" at /Users/macpro/.julia/v0.6/Combinatorics/src/youngdiagrams.jl:8.\n",
      "Use \"const SkewDiagram = Tuple{Partition,Partition}\" instead.\n"
     ]
    }
   ],
   "source": [
    "# Import Distributions to generate random numbers W matrix of the RBM\n",
    "using Distributions\n",
    "using MNIST\n",
    "using BenchmarkTools\n",
    "using Combinatorics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mArray{T}(::Type{T}, m::Int, n::Int) is deprecated, use Array{T}(m, n) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:64\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mArray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Float64}, ::Int64, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:51\u001b[22m\u001b[22m\n",
      " [3] \u001b[1mtraindata\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/MNIST/src/MNIST.jl:88\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:485\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/IJulia/src/execute_request.jl:157\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/IJulia/src/eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##9#12)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[2], in expression starting on line 1\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mArray{T}(::Type{T}, m::Int) is deprecated, use Array{T}(m) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:64\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mArray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Float64}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./deprecated.jl:51\u001b[22m\u001b[22m\n",
      " [3] \u001b[1mtraindata\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/MNIST/src/MNIST.jl:89\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:485\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/IJulia/src/execute_request.jl:157\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/IJulia/src/eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##9#12)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[2], in expression starting on line 1\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m"
     ]
    }
   ],
   "source": [
    "X_train, y_train = MNIST.traindata()\n",
    "X_test, y_test = MNIST.testdata()\n",
    "\n",
    "T = Float32\n",
    "X_train = Array{T}((X_train - minimum(X_train))/(maximum(X_train) - minimum(X_train)))\n",
    "y_train = Array{T}(y_train)\n",
    "X_test = Array{T}(X_test - minimum(X_test))/(maximum(X_test) - minimum(X_test)) \n",
    "y_test = Array{T}(y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define basic types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_params! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(x::Float32)\n",
    "    return 1/(1 + exp(-x))\n",
    "end\n",
    "\n",
    "type RBM{T <: Real}\n",
    "    n_vis::Int\n",
    "    n_hid::Int\n",
    "    W::Matrix{T}  \n",
    "    vis_bias::Vector{T}     \n",
    "    hid_bias::Vector{T}   \n",
    "    trained::Bool\n",
    "    n_epochs_trained::Int\n",
    "end\n",
    "\n",
    "function initialize_RBM(n_vis, n_hid, sigma, T)\n",
    "    \n",
    "    return RBM{T}(n_vis,                                   # num visible units \n",
    "                  n_hid,                                   # num hidden unnits\n",
    "                  rand(Normal(0,sigma), n_hid, n_vis),     # weight matrix\n",
    "                  zeros(n_vis),                            # visible vector  \n",
    "                  zeros(n_hid),                            # Hidden vector\n",
    "                  false,0)                                 # trained\n",
    "end\n",
    "\n",
    "function Base.show{T}(io::IO, rbm::RBM{T})\n",
    "    n_vis = size(rbm.vis_bias, 1)\n",
    "    n_hid = size(rbm.hid_bias, 1)\n",
    "    trained = rbm.trained\n",
    "    print(io, \"RBM{$T}(n_vis=$n_vis, n_hid=$n_hid, trained=$trained)\")\n",
    "end\n",
    "\n",
    "\n",
    "type CDK{T}\n",
    "    K::Int\n",
    "    batch_size::Int\n",
    "    \n",
    "    # Placeholders needed for the gradients of the parameters of the RBM\n",
    "    grad_W::Matrix{T}         \n",
    "    grad_vis_bias::Vector{T}     \n",
    "    grad_hid_bias::Vector{T}   \n",
    "    \n",
    "    # Placeholders needed for performing CDK in a minibatch\n",
    "    H::Matrix{T}\n",
    "    V_hat::Matrix{T}\n",
    "    H_hat::Matrix{T}\n",
    "    rec_error::Float64 # This is probably irrelevant, allo\n",
    "    \n",
    "    # Placeholders needed for performing sampling in a minibatch\n",
    "    V_sampling::Matrix{T}\n",
    "    H_sampling::Matrix{T}   \n",
    "    H_aux::Matrix{T}  \n",
    "    V_aux::Matrix{T}  \n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "function initialize_CDK(rbm::RBM, K, batch_size)\n",
    "    \"\"\"\n",
    "    This function initializes a CDK type that will be used as placeholder for the\n",
    "    memory needed for the gibbs sampling process needed at every minibatch update.\n",
    "    \"\"\"\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    grad_W = zeros(T, size(rbm.W))\n",
    "    grad_vis_bias = zeros(T, size(rbm.vis_bias))\n",
    "    grad_hid_bias = zeros(T, size(rbm.hid_bias))\n",
    "    V_hat = zeros(T, rbm.n_vis, batch_size)\n",
    "    H_hat = zeros(T, rbm.n_hid, batch_size)\n",
    "    H = zeros(T, rbm.n_hid, batch_size)\n",
    "    V_sampling = zeros(T, rbm.n_vis, batch_size)\n",
    "    H_sampling = zeros(T, rbm.n_hid, batch_size)\n",
    "    H_aux = zeros(T, rbm.n_hid, batch_size)\n",
    "    V_aux = zeros(T, rbm.n_vis, batch_size)\n",
    "\n",
    "    cdk = CDK(K, batch_size, \n",
    "              grad_W, grad_vis_bias,grad_hid_bias,\n",
    "              H, V_hat, H_hat, 0.,\n",
    "              V_sampling, H_sampling, H_aux,V_aux)\n",
    "    return cdk\n",
    "end\n",
    "\n",
    "function update_params!(rbm::RBM, opt::CDK, lr)\n",
    "    rbm.W .+= lr .* opt.grad_W \n",
    "    rbm.vis_bias .+= lr .* opt.grad_vis_bias\n",
    "    rbm.hid_bias .+= lr .* opt.grad_hid_bias\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fit without .= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function fit!(rbm::RBM, \n",
    "              X::Matrix, \n",
    "              batch_size::Integer,\n",
    "              n_epochs::Integer,\n",
    "              lr::Real,\n",
    "              shuffle_data::Bool,\n",
    "              opt)\n",
    "        \n",
    "    T = eltype(X)\n",
    "    lr = T(lr)\n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    sample_perm = Vector(1:n_samples)\n",
    "    n_minibatches = T(length(indicies))\n",
    "    rec_errors = Vector{T}([])\n",
    "        \n",
    "    ###### Initialize Optimizer, CDK, PCDK, ....#######\n",
    "    #cdk = initialize_CDK(rbm, K, batch_size)  \n",
    "    \n",
    "    for epoch in 1:n_epochs\n",
    "        rec_error = Float32(0.)\n",
    "        \n",
    "        # should  it be more efficient to Shuffle indicies not the whole data?\n",
    "        # then access is not contiguous though\n",
    "        if shuffle_data==true\n",
    "            shuffle!(sample_perm)\n",
    "            X .= X[:,sample_perm]\n",
    "        end\n",
    "        \n",
    "        for minibatch_ind in indicies          \n",
    "            partial_fit!(rbm, X[:, minibatch_ind], lr, opt)\n",
    "            rec_error += opt.rec_error\n",
    "        end\n",
    "        \n",
    "        push!(rec_errors, rec_error/n_minibatches)\n",
    "        rbm.n_epochs_trained +=1\n",
    "        print(rec_errors[end], \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "    return rec_errors\n",
    "end\n",
    "\n",
    "function partial_fit!(rbm::RBM, X::Matrix,  lr::Real, opt::CDK)\n",
    "    compute_grad!(rbm, X, opt)\n",
    "    update_params!(rbm, opt, lr)    \n",
    "end\n",
    "\n",
    "function compute_grad!(rbm::RBM, X::Matrix,  opt::CDK)\n",
    "\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    batch_size = size(X)[2]\n",
    "    \n",
    "    # Perform gibbs sampling to compute the negative phase\n",
    "    for k in 1:opt.K\n",
    "        if k==1       \n",
    "            opt.H .= sigmoid.(rbm.W * X .+ rbm.hid_bias)\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H .+ rbm.vis_bias) .> rand(T,rbm.n_vis, batch_size)\n",
    "            opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) \n",
    "        else\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H_hat .+ rbm.vis_bias) .> rand(T,rbm.n_vis, batch_size)\n",
    "            opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) \n",
    "        end               \n",
    "    end   \n",
    "   \n",
    "    opt.grad_W =  (opt.H * X' .-  opt.H_hat * opt.V_hat')./ batch_size; \n",
    "    opt.grad_vis_bias = vec(sum((X .- opt.V_hat), 2))./ batch_size;\n",
    "    opt.grad_hid_bias = vec(sum((opt.H .- opt.H_hat), 2))./ batch_size;\n",
    "    \n",
    "    opt.rec_error = sqrt(sum((X.-opt.V_hat).^2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@time A_mul_B!(cdk.H_hat,rbm.W, X_train[:,1:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 20, 0.01, Float32);\n",
    "cdk = initialize_CDK(rbm, 2, 500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  10.81 MiB\n",
       "  allocs estimate:  61\n",
       "  --------------\n",
       "  minimum time:     13.835 ms (0.00% GC)\n",
       "  median time:      14.890 ms (0.00% GC)\n",
       "  mean time:        15.344 ms (2.18% GC)\n",
       "  maximum time:     24.032 ms (3.29% GC)\n",
       "  --------------\n",
       "  samples:          324\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function partial_fit!(rbm::RBM, X::Matrix, K::Integer, lr::Real, optimizer::CDK)\n",
    "@benchmark partial_fit!(rbm, X_train[:,1:500], 0.1, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.7846\n",
      "209.95836\n",
      "208.93597\n",
      "208.024\n",
      "207.10504\n",
      "206.2031\n",
      "205.46944\n",
      "204.74767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.27 GiB\n",
       "  allocs estimate:  7929\n",
       "  --------------\n",
       "  minimum time:     1.758 s (2.15% GC)\n",
       "  median time:      1.790 s (2.11% GC)\n",
       "  mean time:        1.781 s (2.13% GC)\n",
       "  maximum time:     1.797 s (2.10% GC)\n",
       "  --------------\n",
       "  samples:          3\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "@benchmark fit!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using .= to update the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_grad_with_dot! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function fit_with_dot!(rbm::RBM, \n",
    "              X::Matrix, \n",
    "              batch_size::Integer,\n",
    "              n_epochs::Integer,\n",
    "              lr::Real,\n",
    "              shuffle_data::Bool,\n",
    "              opt)\n",
    "        \n",
    "    T = eltype(X)\n",
    "    lr = T(lr)\n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    sample_perm = Vector(1:n_samples)\n",
    "    n_minibatches = T(length(indicies))\n",
    "    rec_errors = Vector{T}([])\n",
    "            \n",
    "    for epoch in 1:n_epochs\n",
    "        rec_error = Float32(0.)\n",
    "        \n",
    "        # should  it be more efficient to Shuffle indicies not the whole data?\n",
    "        # then access is not contiguous though\n",
    "        if shuffle_data==true\n",
    "            shuffle!(sample_perm)\n",
    "            X .= X[:,sample_perm]\n",
    "        end\n",
    "        \n",
    "        for minibatch_ind in indicies          \n",
    "            partial_fit_with_dot!(rbm, X[:, minibatch_ind], lr, opt)\n",
    "            rec_error += opt.rec_error\n",
    "        end\n",
    "        \n",
    "        push!(rec_errors, rec_error/n_minibatches)\n",
    "        rbm.n_epochs_trained +=1\n",
    "        print(rec_errors[end], \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "    return rec_errors\n",
    "end\n",
    "\n",
    "function partial_fit_with_dot!(rbm::RBM, X::Matrix,  lr::Real, opt::CDK)\n",
    "    compute_grad_with_dot!(rbm, X, opt)\n",
    "    update_params!(rbm, opt, lr)    \n",
    "end\n",
    "\n",
    "function compute_grad_with_dot!(rbm::RBM, X::Matrix,  opt::CDK)\n",
    "\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    batch_size = size(X)[2]\n",
    "    \n",
    "    # Perform gibbs sampling to compute the negative phase\n",
    "    for k in 1:opt.K\n",
    "        opt.V_sampling .= rand(T, rbm.n_vis, batch_size)\n",
    "        \n",
    "        if k==1       \n",
    "            opt.H .= sigmoid.(rbm.W * X .+ rbm.hid_bias)\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H .+ rbm.vis_bias) .> opt.V_sampling\n",
    "            opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) \n",
    "        else\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H_hat .+ rbm.vis_bias) .> opt.V_sampling\n",
    "            opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) \n",
    "        end               \n",
    "    end   \n",
    "   \n",
    "    opt.grad_W .=  (opt.H * X' .-  opt.H_hat * opt.V_hat')./ batch_size; \n",
    "    opt.grad_vis_bias .= vec(sum((X .- opt.V_hat), 2))./ batch_size;\n",
    "    opt.grad_hid_bias .= vec(sum((opt.H .- opt.H_hat), 2))./ batch_size;\n",
    "    opt.rec_error = sqrt(sum((X .- opt.V_hat).^2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 20, 0.01, Float32);\n",
    "cdk = initialize_CDK(rbm, 2, 500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  10.75 MiB\n",
       "  allocs estimate:  57\n",
       "  --------------\n",
       "  minimum time:     14.147 ms (0.00% GC)\n",
       "  median time:      15.545 ms (0.00% GC)\n",
       "  mean time:        15.688 ms (2.21% GC)\n",
       "  maximum time:     21.027 ms (3.58% GC)\n",
       "  --------------\n",
       "  samples:          317\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark partial_fit_with_dot!(rbm, X_train[:,1:500], 0.1, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.84888\n",
      "209.96886\n",
      "208.89296\n",
      "208.09567\n",
      "207.08444\n",
      "206.23053\n",
      "205.44463\n",
      "204.65509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.26 GiB\n",
       "  allocs estimate:  7458\n",
       "  --------------\n",
       "  minimum time:     1.868 s (2.10% GC)\n",
       "  median time:      1.895 s (2.05% GC)\n",
       "  mean time:        1.897 s (2.05% GC)\n",
       "  maximum time:     1.928 s (2.02% GC)\n",
       "  --------------\n",
       "  samples:          3\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "@benchmark fit_with_dot!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making propup inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inplace matrix multiplication\n",
    "\n",
    "**A_mul_B!(Y, A, B) → Y**\n",
    "\n",
    "Calculates the matrix-matrix or matrix-vector product 𝐴 · 𝐵 and stores the result in Y, overwriting the existing value of Y. Note that Y must not be aliased with either A or B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_grad_with_dot2! (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function fit_with_dot2!(rbm::RBM, \n",
    "              X::Matrix, \n",
    "              batch_size::Integer,\n",
    "              n_epochs::Integer,\n",
    "              lr::Real,\n",
    "              shuffle_data::Bool,\n",
    "              opt)\n",
    "        \n",
    "    T = eltype(X)\n",
    "    lr = T(lr)\n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    sample_perm = Vector(1:n_samples)\n",
    "    n_minibatches = T(length(indicies))\n",
    "    rec_errors = Vector{T}([])\n",
    "            \n",
    "    for epoch in 1:n_epochs\n",
    "        rec_error = Float32(0.)\n",
    "        \n",
    "        # should  it be more efficient to Shuffle indicies not the whole data?\n",
    "        # then access is not contiguous though\n",
    "        if shuffle_data==true\n",
    "            shuffle!(sample_perm)\n",
    "            X .= X[:,sample_perm]\n",
    "        end\n",
    "        \n",
    "        for minibatch_ind in indicies          \n",
    "            partial_fit_with_dot2!(rbm, X[:, minibatch_ind], lr, opt)\n",
    "            rec_error += opt.rec_error\n",
    "        end\n",
    "        \n",
    "        push!(rec_errors, rec_error/n_minibatches)\n",
    "        rbm.n_epochs_trained +=1\n",
    "        print(rec_errors[end], \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "    return rec_errors\n",
    "end\n",
    "\n",
    "function partial_fit_with_dot2!(rbm::RBM, X::Matrix,  lr::Real, opt::CDK)\n",
    "    compute_grad_with_dot2!(rbm, X, opt)\n",
    "    update_params!(rbm, opt, lr)    \n",
    "end\n",
    "\n",
    "function compute_grad_with_dot2!(rbm::RBM, X::Matrix,  opt::CDK)\n",
    "\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    batch_size = size(X)[2]\n",
    "    \n",
    "    # Perform gibbs sampling to compute the negative phase\n",
    "    for k in 1:opt.K\n",
    "        #opt.V_sampling .= rand(T, rbm.n_vis, batch_size)\n",
    "        rand!(opt.V_sampling)\n",
    "        \n",
    "        if k==1       \n",
    "            opt.H .= sigmoid.(A_mul_B!(opt.H_aux, rbm.W, X) .+ rbm.hid_bias)\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H .+ rbm.vis_bias) .> opt.V_sampling\n",
    "            opt.H_hat .= sigmoid.(A_mul_B!(opt.H_aux, rbm.W, opt.V_hat)  .+ rbm.hid_bias) \n",
    "        else\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H_hat .+ rbm.vis_bias) .> opt.V_sampling\n",
    "            opt.H_hat .= sigmoid.(A_mul_B!(opt.H_aux, rbm.W, opt.V_hat)  .+ rbm.hid_bias) \n",
    "        end        \n",
    "    end   \n",
    "   \n",
    "    opt.grad_W .=  (opt.H * X' .-  opt.H_hat * opt.V_hat')./ batch_size; \n",
    "    opt.grad_vis_bias .= vec(sum((X .- opt.V_hat), 2))./ batch_size;\n",
    "    opt.grad_hid_bias .= vec(sum((opt.H .- opt.H_hat), 2))./ batch_size;\n",
    "    opt.rec_error = sqrt(sum((X .- opt.V_hat).^2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 20, 0.01, Float32);\n",
    "cdk = initialize_CDK(rbm, 2, 500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  7.64 MiB\n",
       "  allocs estimate:  47\n",
       "  --------------\n",
       "  minimum time:     13.719 ms (0.00% GC)\n",
       "  median time:      14.347 ms (0.00% GC)\n",
       "  mean time:        14.557 ms (1.61% GC)\n",
       "  maximum time:     23.497 ms (3.61% GC)\n",
       "  --------------\n",
       "  samples:          342\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark partial_fit_with_dot2!(rbm, X_train[:,1:500], 0.1, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.84149\n",
      "208.83263\n",
      "207.87555\n",
      "206.95923\n",
      "206.08691\n",
      "205.26257\n",
      "204.49213\n",
      "203.83333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  917.24 MiB\n",
       "  allocs estimate:  6259\n",
       "  --------------\n",
       "  minimum time:     1.736 s (1.49% GC)\n",
       "  median time:      1.751 s (1.48% GC)\n",
       "  mean time:        1.755 s (1.48% GC)\n",
       "  maximum time:     1.779 s (1.48% GC)\n",
       "  --------------\n",
       "  samples:          3\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "@benchmark fit_with_dot2!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203.11678\n",
      "202.40695\n",
      "201.84895\n",
      "201.26213\n",
      "200.67061\n",
      "200.11891\n",
      "199.67332\n",
      "199.16504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  917.24 MiB\n",
       "  allocs estimate:  6258\n",
       "  --------------\n",
       "  minimum time:     1.716 s (1.50% GC)\n",
       "  median time:      1.725 s (1.49% GC)\n",
       "  mean time:        1.724 s (1.49% GC)\n",
       "  maximum time:     1.732 s (1.48% GC)\n",
       "  --------------\n",
       "  samples:          3\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "@benchmark fit_with_dot2!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expand(:(A_mul_Bt(cdk.H, X_train[:,1:500])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expand(:(cdk.H*X_train[:,1:500]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholders for all computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_grad_with_dot3! (generic function with 1 method)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function fit_with_dot3!(rbm::RBM, \n",
    "              X::Matrix, \n",
    "              batch_size::Integer,\n",
    "              n_epochs::Integer,\n",
    "              lr::Real,\n",
    "              shuffle_data::Bool,\n",
    "              opt)\n",
    "        \n",
    "    T = eltype(X)\n",
    "    lr = T(lr)\n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    sample_perm = Vector(1:n_samples)\n",
    "    n_minibatches = T(length(indicies))\n",
    "    rec_errors = Vector{T}([])\n",
    "            \n",
    "    for epoch in 1:n_epochs\n",
    "        rec_error = Float32(0.)\n",
    "        \n",
    "        # should  it be more efficient to Shuffle indicies not the whole data?\n",
    "        # then access is not contiguous though\n",
    "        if shuffle_data==true\n",
    "            shuffle!(sample_perm)\n",
    "            X .= X[:,sample_perm]\n",
    "        end\n",
    "        \n",
    "        for minibatch_ind in indicies          \n",
    "            partial_fit_with_dot2!(rbm, X[:, minibatch_ind], lr, opt)\n",
    "            rec_error += opt.rec_error\n",
    "        end\n",
    "        \n",
    "        push!(rec_errors, rec_error/n_minibatches)\n",
    "        rbm.n_epochs_trained +=1\n",
    "        print(rec_errors[end], \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "    return rec_errors\n",
    "end\n",
    "\n",
    "function partial_fit_with_dot3!(rbm::RBM, X::Matrix,  lr::Real, opt::CDK)\n",
    "    compute_grad_with_dot3!(rbm, X, opt)\n",
    "    update_params!(rbm, opt, lr)    \n",
    "end\n",
    "\n",
    "function compute_grad_with_dot3!(rbm::RBM, X::Matrix,  opt::CDK)\n",
    "\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    batch_size = size(X)[2]\n",
    "    \n",
    "    # Perform gibbs sampling to compute the negative phase\n",
    "    for k in 1:opt.K\n",
    "        #opt.V_sampling .= rand(T, rbm.n_vis, batch_size)\n",
    "        rand!(opt.V_sampling)\n",
    "        \n",
    "        if k==1       \n",
    "            opt.H .= sigmoid.(A_mul_B!(opt.H_aux, rbm.W, X) .+ rbm.hid_bias)\n",
    "            opt.V_hat .= sigmoid.(At_mul_B!(opt.V_aux, rbm.W, opt.H).+ rbm.vis_bias) .> opt.V_sampling\n",
    "            opt.H_hat .= sigmoid.(A_mul_B!(opt.H_aux, rbm.W, opt.V_hat)  .+ rbm.hid_bias) \n",
    "        else\n",
    "            opt.V_hat .= sigmoid.(At_mul_B!(opt.V_hat, rbm.W, opt.H_hat) .+ rbm.vis_bias) .> opt.V_sampling\n",
    "            opt.H_hat .= sigmoid.(A_mul_B!(opt.H_hat, rbm.W, opt.V_hat)  .+ rbm.hid_bias) \n",
    "        end        \n",
    "    end   \n",
    "   \n",
    "    # opt.grad_W .=  (opt.H * X' .-  opt.H_hat * opt.V_hat')./ batch_size; \n",
    "    opt.grad_W .=  (A_mul_Bt!(opt.grad_W, opt.H , X) .-  A_mul_Bt!(opt.grad_W, opt.H_hat , opt.V_hat))./ batch_size; \n",
    "    opt.grad_vis_bias .= vec(sum((X .- opt.V_hat), 2))./ batch_size;\n",
    "    opt.grad_hid_bias .= vec(sum((opt.H .- opt.H_hat), 2))./ batch_size;\n",
    "    opt.rec_error = sqrt(sum((X .- opt.V_hat).^2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 20, 0.01, Float32);\n",
    "cdk = initialize_CDK(rbm, 2, 500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 784)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(A_mul_Bt( cdk.H ,  X_train[:,1:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  4.53 MiB\n",
       "  allocs estimate:  39\n",
       "  --------------\n",
       "  minimum time:     13.070 ms (0.00% GC)\n",
       "  median time:      13.357 ms (0.00% GC)\n",
       "  mean time:        14.156 ms (1.40% GC)\n",
       "  maximum time:     23.972 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          351\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark partial_fit_with_dot3!(rbm, X_train[:,1:500], 0.1, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDimensionMismatch(\"A has dimensions (20,500) but B has dimensions (784,500)\")\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDimensionMismatch(\"A has dimensions (20,500) but B has dimensions (784,500)\")\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mgemm_wrapper!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float32,2}, ::Char, ::Char, ::Array{Float32,2}, ::Array{Float32,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:347\u001b[22m\u001b[22m",
      " [2] \u001b[1mA_mul_Bt!\u001b[22m\u001b[22m at \u001b[1m./linalg/matmul.jl:193\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1mcompute_grad_with_dot2!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::RBM{Float32}, ::Array{Float32,2}, ::CDK{Float32}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[18]:66\u001b[22m\u001b[22m",
      " [4] \u001b[1mpartial_fit_with_dot2!\u001b[22m\u001b[22m at \u001b[1m./In[13]:42\u001b[22m\u001b[22m [inlined]",
      " [5] \u001b[1mfit_with_dot3!\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::RBM{Float32}, ::Array{Float32,2}, ::Int64, ::Int64, ::Float64, ::Bool, ::CDK{Float32}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[86]:29\u001b[22m\u001b[22m",
      " [6] \u001b[1m##core#1298\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/BenchmarkTools/src/execution.jl:283\u001b[22m\u001b[22m",
      " [7] \u001b[1m##sample#1299\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/BenchmarkTools/src/execution.jl:289\u001b[22m\u001b[22m",
      " [8] \u001b[1m#_run#333\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Bool, ::String, ::Array{Any,1}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/BenchmarkTools/src/execution.jl:317\u001b[22m\u001b[22m",
      " [9] \u001b[1m(::BenchmarkTools.#kw##_run)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::BenchmarkTools.#_run, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [10] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m",
      " [11] \u001b[1m#run_result#16\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/BenchmarkTools/src/execution.jl:33\u001b[22m\u001b[22m",
      " [12] \u001b[1m(::BenchmarkTools.#kw##run_result)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::BenchmarkTools.#run_result, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [13] \u001b[1m#run#17\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/BenchmarkTools/src/execution.jl:36\u001b[22m\u001b[22m",
      " [14] \u001b[1m(::Base.#kw##run)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Base.#run, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [15] \u001b[1mwarmup\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::BenchmarkTools.Benchmark{Symbol(\"##benchmark#1297\")}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/macpro/.julia/v0.6/BenchmarkTools/src/execution.jl:71\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "@benchmark fit_with_dot3!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.0-pre.alpha",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
