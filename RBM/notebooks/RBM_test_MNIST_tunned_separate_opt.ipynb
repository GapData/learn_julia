{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Distributions to generate random numbers W matrix of the RBM\n",
    "using Distributions\n",
    "using MNIST\n",
    "using BenchmarkTools\n",
    "using Combinatorics\n",
    "#using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = MNIST.traindata()\n",
    "X_test, y_test = MNIST.testdata()\n",
    "\n",
    "T = Float32\n",
    "X_train = Array{T}( (X_train - minimum(X_train))/(maximum(X_train) - minimum(X_train)) )\n",
    "y_train = Array{T}(y_train)\n",
    "X_test = Array{T}(X_test - minimum(X_test))/(maximum(X_test) - minimum(X_test)) \n",
    "y_test = Array{T}(y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed function return arrays vs modify arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addstuff! (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function addstuff(a, b)\n",
    "    return a .+1, b .+1\n",
    "end\n",
    "\n",
    "function addstuff!(a,b)\n",
    "    a.= a .+ 1\n",
    "    b.= b .+ 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = zeros(10);\n",
    "B = zeros(20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  432 bytes\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     115.934 ns (0.00% GC)\n",
       "  median time:      133.447 ns (0.00% GC)\n",
       "  mean time:        176.786 ns (14.76% GC)\n",
       "  maximum time:     2.709 μs (84.79% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     927\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark res = addstuff(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     50.119 ns (0.00% GC)\n",
       "  median time:      53.290 ns (0.00% GC)\n",
       "  mean time:        63.692 ns (0.00% GC)\n",
       "  maximum time:     1.734 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     986\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark addstuff!(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make RBM with efficient optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partial_fit! (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(x::Float32)\n",
    "    return 1/(1 + exp(-x))\n",
    "end\n",
    "\n",
    "type RBM{T <: Real}\n",
    "    n_vis::Int\n",
    "    n_hid::Int\n",
    "    W::Matrix{T}         \n",
    "    vis_bias::Vector{T}     \n",
    "    hid_bias::Vector{T}   \n",
    "    trained::Bool\n",
    "    n_epochs_trained::Int\n",
    "end\n",
    "\n",
    "function initialize_RBM(n_vis, n_hid, sigma, T)\n",
    "    \n",
    "    return RBM{T}(n_vis,                                 # num visible units \n",
    "                  n_hid,                                 # num hidden unnits\n",
    "                  rand(Normal(0,sigma), n_hid, n_vis),  # weight matrix\n",
    "                  zeros(n_vis),                          # visible vector  \n",
    "                  zeros(n_hid),                          # Hidden vector\n",
    "                  false,0)                                 # trained\n",
    "end\n",
    "\n",
    "function Base.show{T}(io::IO, rbm::RBM{T})\n",
    "    n_vis = size(rbm.vis_bias, 1)\n",
    "    n_hid = size(rbm.hid_bias, 1)\n",
    "    trained = rbm.trained\n",
    "    print(io, \"RBM{$T}(n_vis=$n_vis, n_hid=$n_hid, trained=$trained)\")\n",
    "end\n",
    "\n",
    "function generate_M(W, n_columns)\n",
    "\n",
    "    n_hid = size(W)[1]\n",
    "    n_vis = size(W)[2]\n",
    "    costat = Int(sqrt(n_vis))    \n",
    "    n_rows = Int(round(n_hid/n_columns))   \n",
    "\n",
    "    print(\"\\ncostat: \",costat,\n",
    "          \"\\nn_rows: \", n_rows,\n",
    "          \"\\nn_cols: \", n_columns)\n",
    "    \n",
    "    M = zeros(costat * n_rows, costat * n_columns)\n",
    "    \n",
    "    n_im = 1\n",
    "    for r in 1:costat:size(M)[1]\n",
    "        for c in 1:costat:size(M)[2]\n",
    "            M[r:r+costat-1, c:c+costat-1] = reshape(W[n_im,:],costat,costat)\n",
    "            n_im +=1\n",
    "        end\n",
    "    end\n",
    "    return M\n",
    "end\n",
    "\n",
    "#    # Placeholders needed for the gradients of the parameters of the RBM\n",
    "#    grad_W::Matrix{T}         \n",
    "#    grad_vis_bias::Vector{T}     \n",
    "#    grad_hid_bias::Vector{T}   \n",
    "#    \n",
    "#    # Placeholders needed for performing CDK in a minibatch\n",
    "#    #V::Matrix{T}\n",
    "#    H::Matrix{T}\n",
    "#    V_hat::Matrix{T}\n",
    "#    H_hat::Matrix{T}\n",
    "\n",
    "type CDK{T}\n",
    "    K::Int\n",
    "    batch_size::Int\n",
    "    \n",
    "    # Placeholders needed for the gradients of the parameters of the RBM\n",
    "    grad_W::Matrix{T}         \n",
    "    grad_vis_bias::Vector{T}     \n",
    "    grad_hid_bias::Vector{T}   \n",
    "    \n",
    "    # Placeholders needed for performing CDK in a minibatch\n",
    "    H::Matrix{T}\n",
    "    V_hat::Matrix{T}\n",
    "    H_hat::Matrix{T}\n",
    "    rec_error::Float64\n",
    "\n",
    "    # Placeholders needed for performing sampling in a minibatch\n",
    "    V_sampling::Matrix{T}\n",
    "    H_sampling::Matrix{T}    \n",
    "    \n",
    "end\n",
    "\n",
    "function initialize_CDK(rbm::RBM, K, batch_size)\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    \n",
    "    grad_W = zeros(T, rbm.W)\n",
    "    grad_vis_bias = zeros(T, rbm.vis_bias)\n",
    "    grad_hid_bias = zeros(T, rbm.hid_bias)\n",
    "    V_hat = zeros(T, rbm.n_vis, batch_size)\n",
    "    H_hat = zeros(T, rbm.n_hid, batch_size)\n",
    "    H = zeros(T, rbm.n_hid, batch_size)\n",
    "    V_sampling = zeros(T, rbm.n_vis, batch_size)\n",
    "    H_sampling = zeros(T, rbm.n_hid, batch_size)\n",
    "    \n",
    "    cdk = CDK(K, batch_size, \n",
    "              grad_W, grad_vis_bias,grad_hid_bias,\n",
    "              H, V_hat, H_hat, 0.,\n",
    "              V_sampling, H_sampling)\n",
    "    \n",
    "    return cdk\n",
    "end\n",
    "\n",
    "function fit!(rbm::RBM, X::Matrix, batch_size::Integer, n_epochs::Integer, lr::Real, shuffle_data::Bool, opt)\n",
    "        \n",
    "    T = eltype(X)\n",
    "    lr = T(lr)\n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    sample_perm = Vector(1:n_samples)\n",
    "    n_minibatches = T(length(indicies))\n",
    "    rec_errors = Vector{T}([])\n",
    "        \n",
    "    ###### Initialize Optimizer, CDK, PCDK, ....#######\n",
    "    #cdk = initialize_CDK(rbm, K, batch_size)  \n",
    "    \n",
    "    for epoch in 1:n_epochs\n",
    "        rec_error = Float32(0.)\n",
    "        \n",
    "        # should  it be more efficient to Shuffle indicies not the whole data?\n",
    "        # then access is not contiguous though\n",
    "        if shuffle_data==true\n",
    "            shuffle!(sample_perm)\n",
    "            X .= X[:,sample_perm]\n",
    "        end\n",
    "        \n",
    "        for minibatch_ind in indicies          \n",
    "            partial_fit!(rbm, X[:, minibatch_ind], lr, opt)\n",
    "            rec_error += opt.rec_error\n",
    "        end\n",
    "        \n",
    "        push!(rec_errors, rec_error/n_minibatches)\n",
    "        rbm.n_epochs_trained +=1\n",
    "        print(rec_errors[end], \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "    return rec_errors\n",
    "end\n",
    "\n",
    "function partial_fit!(rbm::RBM, X::Matrix,  lr::Real, opt::CDK)\n",
    "    #lr = eltype(rbm.vis_bias)(lr)\n",
    "    compute_grad!(rbm, X, opt)\n",
    "    update_params!(rbm, opt, lr)    \n",
    "end\n",
    "\n",
    "\n",
    "# function grad_apply_momentum!{T}(rbm::RBM{T}, X::Mat,\n",
    "#                                  dtheta::Tuple, ctx::Dict)\n",
    "#     dW, db, dc = dtheta\n",
    "#     momentum = @get(ctx, :momentum, 0.9)\n",
    "#     dW_prev = @get_array(ctx, :dW_prev, size(dW), zeros(T, size(dW)))\n",
    "#     # same as: dW += momentum * dW_prev\n",
    "#     axpy!(momentum, dW_prev, dW)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_params! (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_grad!(rbm::RBM, X::Matrix,  opt::CDK)\n",
    "\n",
    "    T = eltype(rbm.vis_bias)\n",
    "    batch_size = size(X)[2]\n",
    "    \n",
    "    # Add code here that copies some instances of X to make it size of the \"chosen minibatch\"\n",
    "    # in case the last minibatch has not the same size as all other minibatches. \n",
    "    # We want to do this to avoid destroying all the memory allocations in opt and creating them again\n",
    "\n",
    "    # Perform gibbs sampling to compute the negative phase\n",
    "    for k in 1:opt.K\n",
    "        if k==1       \n",
    "            opt.H .= sigmoid.(rbm.W * X .+ rbm.hid_bias)\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H .+ rbm.vis_bias) .> rand(T,rbm.n_vis, batch_size)\n",
    "            opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) \n",
    "        else\n",
    "            opt.V_hat .= sigmoid.(rbm.W'* opt.H_hat .+ rbm.vis_bias) .> rand(T,rbm.n_vis, batch_size)\n",
    "            opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) \n",
    "        end      \n",
    "        ## This does not impact that much\n",
    "        #opt.V_sampling .= rand(T, rbm.n_vis, batch_size)\n",
    "        #opt.H_sampling .= rand(T, rbm.n_hid, batch_size)\n",
    "        #\n",
    "        #if k==1       \n",
    "        #    opt.H .= sigmoid.(rbm.W * X .+ rbm.hid_bias)\n",
    "        #    opt.V_hat .= sigmoid.(rbm.W'* opt.H .+ rbm.vis_bias) .> opt.V_sampling\n",
    "        #    opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) .> opt.H_sampling\n",
    "        #else\n",
    "        #    opt.V_hat .= sigmoid.(rbm.W'* opt.H_hat .+ rbm.vis_bias) .> opt.V_sampling\n",
    "        #    opt.H_hat .= sigmoid.(rbm.W * opt.V_hat .+ rbm.hid_bias) .> opt.H_sampling\n",
    "        #end               \n",
    "    end   \n",
    "   \n",
    "    # WORKS\n",
    "    opt.grad_W = (opt.H * X' .-  opt.H_hat * opt.V_hat')./ batch_size; \n",
    "    opt.grad_vis_bias = vec(sum((X .- opt.V_hat), 2))./ batch_size;\n",
    "    opt.grad_hid_bias = vec(sum((opt.H .- opt.H_hat), 2))./ batch_size;\n",
    "    \n",
    "    # DOES NOT WORK (with .=) WHY?????\n",
    "    #opt.grad_W .=  (opt.H * X' .-  opt.H_hat * opt.V_hat')./ batch_size; \n",
    "    #opt.grad_vis_bias .= vec(sum((X .- opt.V_hat), 2))./ batch_size;\n",
    "    #opt.grad_hid_bias .= vec(sum((opt.H .- opt.H_hat), 2))./ batch_size;\n",
    "\n",
    "    opt.rec_error = sqrt(sum((X.-opt.V_hat).^2))\n",
    "end\n",
    "\n",
    "function update_params!(rbm::RBM, opt::CDK, lr)\n",
    "    rbm.W .+= lr .* opt.grad_W \n",
    "    rbm.vis_bias .+= lr .* opt.grad_vis_bias\n",
    "    rbm.hid_bias .+= lr .* opt.grad_hid_bias\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 20, 0.01, Float32);\n",
    "#cdk = initialize_CDK(rbm, 1, 500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: Cannot `convert` an object of type Array{Float32,2} to an object of type Int64\nThis may have arisen from a call to the constructor Int64(...),\nsince type constructors fall back to convert methods.\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: Cannot `convert` an object of type Array{Float32,2} to an object of type Int64\nThis may have arisen from a call to the constructor Int64(...),\nsince type constructors fall back to convert methods.\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mzeros\u001b[22m\u001b[22m at \u001b[1m./array.jl:215\u001b[22m\u001b[22m [inlined]",
      " [2] \u001b[1mzeros\u001b[22m\u001b[22m at \u001b[1m./array.jl:217\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1minitialize_CDK\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::RBM{Float32}, ::Int64, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[34]:90\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "initialize_CDK(rbm, 1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: cdk not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: cdk not defined\u001b[39m",
      ""
     ]
    }
   ],
   "source": [
    "@time partial_fit!(rbm, X_train[:,1:500], 0.1, cdk);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: cdk not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: cdk not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m##core#321\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/david/.julia/v0.6/BenchmarkTools/src/execution.jl:283\u001b[22m\u001b[22m",
      " [2] \u001b[1m##sample#322\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/david/.julia/v0.6/BenchmarkTools/src/execution.jl:289\u001b[22m\u001b[22m",
      " [3] \u001b[1m#_run#33\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Bool, ::String, ::Array{Any,1}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/david/.julia/v0.6/BenchmarkTools/src/execution.jl:317\u001b[22m\u001b[22m",
      " [4] \u001b[1m(::BenchmarkTools.#kw##_run)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::BenchmarkTools.#_run, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [5] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m",
      " [6] \u001b[1m#run_result#16\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/david/.julia/v0.6/BenchmarkTools/src/execution.jl:33\u001b[22m\u001b[22m",
      " [7] \u001b[1m(::BenchmarkTools.#kw##run_result)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::BenchmarkTools.#run_result, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [8] \u001b[1m#run#17\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/david/.julia/v0.6/BenchmarkTools/src/execution.jl:36\u001b[22m\u001b[22m",
      " [9] \u001b[1m(::Base.#kw##run)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Base.#run, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}, ::BenchmarkTools.Parameters\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [10] \u001b[1mwarmup\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::BenchmarkTools.Benchmark{Symbol(\"##benchmark#320\")}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/david/.julia/v0.6/BenchmarkTools/src/execution.jl:71\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "# function partial_fit!(rbm::RBM, X::Matrix, K::Integer, lr::Real, optimizer::CDK)\n",
    "@benchmark partial_fit!(rbm, X_train[:,1:500], 0.1, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "\n",
    "@benchmark fit!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This was with the update inside!\n",
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.05\n",
    "\n",
    "@benchmark fit!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 2\n",
    "lr = 0.001\n",
    "\n",
    "@benchmark fit!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 3\n",
    "lr = 0.001\n",
    "\n",
    "@benchmark fit!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 10\n",
    "lr = 0.001\n",
    "\n",
    "@benchmark fit!(rbm, X_train, batch_size,  n_epochs, lr, false, cdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 500\n",
    "K = 2\n",
    "lr = 0.001\n",
    "\n",
    "res = fit!(rbm, X_train, batch_size,  n_epochs, K, lr, T, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyPlot.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 20, 0.01, Float32)\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 500\n",
    "K = 3\n",
    "lr = 0.001\n",
    "\n",
    "res = fit!(rbm, X_train, batch_size,  n_epochs, K, lr, T, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyPlot.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in 1:1\n",
    "    print(\"HI\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = generate_M(rbm.W,25);\n",
    "PyPlot.imshow(M,\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = generate_M(rbm.W, 10);\n",
    "\n",
    "PyPlot.imshow(M,\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hneg = zeros(rbm.n_hid, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hneg .= sigmoid.(rbm.W * X_train[:,1:10] .+ rbm.hid_bias) .> rand(rbm.n_hid, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(Hneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/dfdx/Boltzmann.jl/blob/master/examples/mnistexample.jl\n",
    "\n",
    "function plot_weights(W, imsize)\n",
    "    padding=10\n",
    "    h, w = imsize\n",
    "    n = size(W, 1)\n",
    "    rows = Int(floor(sqrt(n)))\n",
    "    cols = Int(ceil(n / rows))\n",
    "    halfpad = div(padding, 2)\n",
    "    dat = zeros(rows * (h + padding), cols * (w + padding))\n",
    "    for i=1:n\n",
    "        wt = W[i, :]\n",
    "        wim = reshape(wt, imsize)\n",
    "        wim = wim ./ (maximum(wim) - minimum(wim))\n",
    "        r = div(i - 1, cols) + 1\n",
    "        c = rem(i - 1, cols) + 1\n",
    "        dat[(r-1)*(h+padding)+halfpad+1 : r*(h+padding)-halfpad,\n",
    "            (c-1)*(w+padding)+halfpad+1 : c*(w+padding)-halfpad] = wim\n",
    "    end\n",
    "    #ImageView.view(dat)\n",
    "    return dat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 50, 0.01, Float32)\n",
    "M = generate_M(rbm.W,25);\n",
    "PyPlot.imshow(M,\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model several epochs \n",
    "\n",
    "Plot the weights after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = initialize_RBM(784, 50, 0.01, Float32)\n",
    "M = generate_M(rbm.W,25);\n",
    "PyPlot.imshow(M,\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "fit!(rbm, X_train,  batch_size,  n_epochs, K, lr, T, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = generate_M(rbm.W, 10);\n",
    "PyPlot.imshow(M,\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyPlot.imshow(M,\"gray\", vmin=minimum(M), vmax = maximum(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = plot_weights(rbm.W[1:50,:], (28,28));\n",
    "PyPlot.imshow(dat,\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scaling individually the features of the plot: TO DO\n",
    "\n",
    "#### Python code for printing the feature detectors\n",
    "\n",
    "\n",
    "    for i, comp in enumerate(self.W.T):\n",
    "        plt.subplot(15, 15, i + 1)\n",
    "        if min_max_scale:\n",
    "            plt.imshow(comp.reshape((28, 28)),\n",
    "                       cmap= plt.get_cmap('gray'), vmin=min_, vmax=max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function plot_per_hidden_unit(W, n_columns, minmax_scale)\n",
    "    n_hid = size(W)[1]\n",
    "    n_vis = size(W)[2]\n",
    "    costat = Int(sqrt(n_vis))    \n",
    "    n_rows = Int(round(n_hid/n_columns))   \n",
    "\n",
    "    print(\"\\ncostat: \", costat,\n",
    "          \"\\nn_rows: \", n_rows,\n",
    "          \"\\nn_cols: \", n_columns)\n",
    "    \n",
    "    if minmax_scale == true\n",
    "        min_ = minimum(W)\n",
    "        max_ = maximum(W)\n",
    "    end\n",
    "                    \n",
    "    for i in 1:n_hid\n",
    "        comp = W[i,:]\n",
    "        PyPlot.subplot(10, 10, i+1)\n",
    "        if minmax_scale == true\n",
    "            PyPlot.imshow(reshape(comp, 28, 28),\n",
    "            cmap= PyPlot.get_cmap(\"gray\"), vmin=min_, vmax=max_)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_per_hidden_unit(rbm.W, rbm.n_hid, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.0-dev",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
