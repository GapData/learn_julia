{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM implementation\n",
    "\n",
    "Objective: Implement CRBM in Julia for time series analysis"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mINFO: Recompiling stale cache file /Users/david/.julia/lib/v0.5/PyPlot.ji for module PyPlot.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Import Distributions to generate random numbers W matrix of the RBM\n",
    "using Distributions\n",
    "using MNIST\n",
    "using BenchmarkTools\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": 3,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "invalid redefinition of constant RBM",
     "output_type": "error",
     "traceback": [
      "invalid redefinition of constant RBM",
      ""
     ]
    }
   ],
   "source": [
    "type RBM{T <: Real}\n",
    "    n_vis::Int\n",
    "    n_hid::Int\n",
    "    W::Matrix{T}         \n",
    "    vis_bias::Vector{T}     \n",
    "    hid_bias::Vector{T}   \n",
    "    trained::Bool\n",
    "end\n",
    "\n",
    "function Base.show{T}(io::IO, rbm::RBM{T})\n",
    "    n_vis = size(rbm.vis_bias, 1)\n",
    "    n_hid = size(rbm.hid_bias, 1)\n",
    "    trained = rbm.trained\n",
    "    print(io, \"RBM{$T}(n_vis=$n_vis, n_hid=$n_hid, trained=$trained)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 4,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition sigmoid(Array{Float64, N<:Any}) in module Main at In[3]:2 overwritten at In[61]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 61,
=======
     "execution_count": 4,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(vector::Array{Float64})\n",
    "    return 1./(1 + exp.(-vector))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
=======
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(x::Float64)\n",
    "    return 1/(1 + exp(-x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition initialize_RBM(Any, Any, Any, Any) in module Main at In[4]:3 overwritten at In[62]:3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "initialize_RBM (generic function with 1 method)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 62,
=======
     "execution_count": 6,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initialize_RBM(n_vis, n_hid, sigma, T)\n",
    "    \n",
    "    return RBM{T}( n_vis,                                 # num visible units \n",
    "                   n_hid,                                 # num hidden unnits\n",
    "                   rand(Normal(0,sigma), n_hid, n_vis),   # weight matrix\n",
    "                   zeros(n_vis),                          # visible vector  \n",
    "                   zeros(n_hid),                          # Hidden vector\n",
    "                   false)                                 # trained\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
=======
   "execution_count": 7,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RBM{Float64}(n_vis=784, n_hid=100, trained=false)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 63,
=======
     "execution_count": 7,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm = initialize_RBM(784, 100, 0.01, Float64)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 64,
=======
   "execution_count": 8,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,784)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 64,
=======
     "execution_count": 8,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(rbm.W)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 66,
=======
   "execution_count": 9,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[7.0,2.0,1.0,0.0,4.0,1.0,4.0,9.0,5.0,9.0  …  7.0,8.0,9.0,0.0,1.0,2.0,3.0,4.0,5.0,6.0])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 66,
=======
     "execution_count": 9,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = MNIST.traindata()\n",
    "X_test, y_test = MNIST.testdata()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,60000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plot (generic function with 1 method)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PyPlot.imshow(reshape(X_train[:,2],28,28));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x3451db6d8>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PyObject <matplotlib.image.AxesImage object at 0x3517a5470>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPlot.matshow(reshape(X_train[:,2],28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
=======
   "execution_count": 10,
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contrastive_divergence_K (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    batch_size = size(Xbatch)[2]\n",
    "    Delta_W = zeros(size(rbm.W))\n",
    "    Delta_b = zeros(size(rbm.vis_bias))\n",
    "    Delta_c = zeros(size(rbm.hid_bias))\n",
    "    \n",
    "    xneg = zeros(size(rbm.vis_bias))\n",
    "    hneg = similar(rbm.hid_bias)\n",
    "    b1 = similar(rbm.W * Xbatch[:,1])\n",
    "    b2 = similar(rbm.W' * hneg)\n",
    "    ehp = similar(rbm.hid_bias)\n",
    "    ehn = similar(rbm.hid_bias)\n",
    "        \n",
    "    @inbounds for i in 1:batch_size\n",
    "        x =  @view Xbatch[:,i]\n",
    "        xneg = @view Xbatch[:,i]\n",
    "\n",
    "        for k in 1:K\n",
    "            hneg .= sigmoid(rbm.W * xneg .+ rbm.hid_bias) .> rand.()\n",
    "            At_mul_B!(b2, rbm.W, hneg)\n",
    "            xneg .= sigmoid(b2 .+ rbm.vis_bias) .> rand.()         \n",
    "        end\n",
    "\n",
    "        A_mul_B!(b1, rbm.W, x)\n",
    "        ehp .= sigmoid(b1 .+ rbm.hid_bias)\n",
    "        A_mul_B!(b1, rbm.W, xneg)\n",
    "        ehn .= sigmoid(b1 .+ rbm.hid_bias)\n",
    "\n",
    "        Delta_W .+= lr .* (ehp .* x' .- ehn .* xneg')\n",
    "        Delta_b .+= lr .* (x .- xneg)\n",
    "        Delta_c .+= lr .* (ehp .- ehn)\n",
    "\n",
    "    end\n",
    "\n",
    "    rbm.W .+= Delta_W ./ batch_size;\n",
    "    rbm.vis_bias .+= Delta_b ./ batch_size;\n",
    "    rbm.hid_bias .+= Delta_c ./ batch_size;\n",
    "\n",
    "    return \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  78.83 MiB\n",
       "  allocs estimate:  3594\n",
       "  --------------\n",
       "  minimum time:     39.530 ms (14.39% GC)\n",
       "  median time:      44.687 ms (17.05% GC)\n",
       "  mean time:        46.280 ms (17.44% GC)\n",
       "  maximum time:     126.115 ms (70.44% GC)\n",
       "  --------------\n",
       "  samples:          108\n",
       "  evals/sample:     1\n",
       "  time tolerance:   5.00%\n",
       "  memory tolerance: 1.00%"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "X_batch = X_train[:,1:25]\n",
    "\n",
    "@benchmark contrastive_divergence_K(X_batch, rbm, 1, 0.01)\n",
    "#@time contrastive_divergence_K(X_batch, rbm, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784,60000),(784,25))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "size(X_train), size(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit RBM"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_CDK (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "function fit_CDK(X, rbm, batch_size::Integer,  n_epochs::Integer, K::Integer, lr::Real)\n",
    "        \n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    mb = 1\n",
    "    print(\"number minibatches:\", length(indicies), \"\\n\")\n",
    "    for epoch in 1:n_epochs\n",
    "        tic();\n",
    "        for minibatch_ind in indicies\n",
    "            Xbatch = @view X[:, minibatch_ind]\n",
    "            contrastive_divergence_K(Xbatch, rbm, K, lr)\n",
    "            \n",
    "        end\n",
    "        print(\"\\nepoch \", epoch, \"  time epoch:\", toq())\n",
    "    end\n",
    "    rbm.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number minibatches:300\n",
      "\n",
      "epoch 1  time epoch:111.04834836111.276573 seconds (8.69 M allocations: 180.973 GB, 15.62% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyPlot.imshow(reshape(rbm.W[9,:],28,28),\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "n_epochs = 1\n",
    "batch_size = 200\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@time fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# vectorized cdk"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition vec_contrastive_divergence_K(Any, Any, Integer, Real) in module Main at In[17]:3 overwritten at In[19]:3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vec_contrastive_divergence_K (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "function vec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    Xneg = copy(Xbatch)\n",
    "    batch_size = size(Xbatch)[2]\n",
    "    \n",
    "    for k in 1:K\n",
    "        Hneg = sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Xneg = sigmoid(rbm.W' * Hneg  .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "       \n",
    "    Ehp = sigmoid(rbm.W * Xbatch .+ rbm.hid_bias)\n",
    "    Ehn = sigmoid(rbm.W * Xneg .+ rbm.hid_bias)\n",
    "\n",
    "    Delta_W = lr*( Ehp * Xbatch' -  Ehn *  Xneg')\n",
    "    Delta_vis_bias = sum(lr .* (Xbatch .- Xneg), 2)[:]\n",
    "    Delta_hid_bias = sum(lr .* (Ehp - Ehn), 2)[:]\n",
    "    \n",
    "    rbm.W .+= Delta_W ./ batch_size;\n",
    "    rbm.vis_bias .+= Delta_vis_bias ./ batch_size;\n",
    "    rbm.hid_bias .+= Delta_hid_bias ./ batch_size;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.018265 seconds (177 allocations: 5.362 MB)\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "X_batch = X_train[:,1:25]\n",
    "#@benchmark vec_contrastive_divergence_K(X_batch, rbm, 1, 0.01)\n",
    "@time vec_contrastive_divergence_K(X_batch, rbm, 1, 0.01);"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition vec_fit_CDK(Any, Any, Integer, Integer, Integer, Real) in module Main at In[24]:3 overwritten at In[27]:3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vec_fit_CDK (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "function vec_fit_CDK(X, rbm, batch_size::Integer,  n_epochs::Integer, K::Integer, lr::Real)\n",
    "        \n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    mb = 1\n",
    "    println(\"number minibatches:\", length(indicies), \"\\n\")\n",
    "    for epoch in 1:n_epochs\n",
    "        tic();\n",
    "        for minibatch_ind in indicies\n",
    "            vec_contrastive_divergence_K((@view X[:, minibatch_ind]), rbm, K, lr)\n",
    "        end\n",
    "        #print(\"\\n\\nepoch \", epoch, \"  time epoch:\", toq(), \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number minibatches:60\n",
      "\n",
      "\n",
      "\n",
      "epoch 1  time epoch:32.401529775\n",
      " 34.229197 seconds (2.10 M allocations: 4.316 GB, 4.73% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> ddf6f89812190792b88659b60392a5be0183e027
   "source": [
    "n_epochs = 20\n",
    "batch_size = 1000\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@time vec_fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number minibatches:300\n",
      "\n",
      "\n",
      "\n",
      "epoch 1  time epoch:29.097666643\n",
      " 29.099399 seconds (87.48 k allocations: 5.120 GB, 1.72% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 200\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@time vec_fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vectorized and optimized\n",
    "\n",
    "- We will avoid making transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":openblas64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLAS.vendor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "gemm(tA, tB, alpha, A, B)\n",
       "```\n",
       "\n",
       "Returns `alpha*A*B` or the other three variants according to `tA` (transpose `A`) and `tB`.\n",
       "\n",
       "```\n",
       "gemm(tA, tB, A, B)\n",
       "```\n",
       "\n",
       "Returns `A*B` or the other three variants according to `tA` (transpose `A`) and `tB`.\n"
      ],
      "text/plain": [
       "```\n",
       "gemm(tA, tB, alpha, A, B)\n",
       "```\n",
       "\n",
       "Returns `alpha*A*B` or the other three variants according to `tA` (transpose `A`) and `tB`.\n",
       "\n",
       "```\n",
       "gemm(tA, tB, A, B)\n",
       "```\n",
       "\n",
       "Returns `A*B` or the other three variants according to `tA` (transpose `A`) and `tB`.\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?BLAS.gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: invalid character literal",
     "output_type": "error",
     "traceback": [
      "syntax: invalid character literal",
      ""
     ]
    }
   ],
   "source": [
    "function vec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    Xneg = copy(Xbatch)\n",
    "    batch_size = size(Xbatch)[2]\n",
    "    \n",
    "    for k in 1:K\n",
    "        Hneg = sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Xneg = sigmoid(rbm.W'* Hneg  .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "    \n",
    "    Ehp = sigmoid(rbm.W * Xbatch .+ rbm.hid_bias)\n",
    "    Ehn = sigmoid(rbm.W * Xneg .+ rbm.hid_bias)\n",
    "    \n",
    "    Delta_W = lr*( Ehp * Xbatch' -  Ehn *  Xneg')\n",
    "    Delta_vis_bias = sum(lr .* (Xbatch .- Xneg), 2)[:]\n",
    "    Delta_hid_bias = sum(lr .* (Ehp - Ehn), 2)[:]\n",
    "    \n",
    "    rbm.W .+= Delta_W ./ batch_size;\n",
    "    rbm.vis_bias .+= Delta_vis_bias ./ batch_size;\n",
    "    rbm.hid_bias .+= Delta_hid_bias ./ batch_size;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching gemm(::Char, ::Char, ::Array{Float64,2}, ::BitArray{2})\u001b[0m\nClosest candidates are:\n  gemm(::Char, ::Char, ::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m) at linalg/blas.jl:984\n  gemm(::Char, ::Char, \u001b[1m\u001b[31m::Float64\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m) at linalg/blas.jl:981\n  gemm(::Char, ::Char, \u001b[1m\u001b[31m::Float32\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float32,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float32,2},SubArray{Float32,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float32,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float32,2},SubArray{Float32,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m) at linalg/blas.jl:981\n  ...\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching gemm(::Char, ::Char, ::Array{Float64,2}, ::BitArray{2})\u001b[0m\nClosest candidates are:\n  gemm(::Char, ::Char, ::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m) at linalg/blas.jl:984\n  gemm(::Char, ::Char, \u001b[1m\u001b[31m::Float64\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float64,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float64,2},SubArray{Float64,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m) at linalg/blas.jl:981\n  gemm(::Char, ::Char, \u001b[1m\u001b[31m::Float32\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float32,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float32,2},SubArray{Float32,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m, \u001b[1m\u001b[31m::Union{Base.ReshapedArray{Float32,2,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray{Float32,2},SubArray{Float32,2,A<:Union{Base.ReshapedArray{T,N,A<:DenseArray,MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N}}},DenseArray},I<:Tuple{Vararg{Union{Base.AbstractCartesianIndex,Colon,Int64,Range{Int64}},N}},L}}\u001b[0m) at linalg/blas.jl:981\n  ...\u001b[0m",
      "",
      " in vecopt_contrastive_divergence_K(::Array{Float64,2}, ::RBM{Float64}, ::Int64, ::Float64) at ./In[55]:8"
     ]
    }
   ],
   "source": [
    "X_batch = X_train[:,1:25]\n",
    "#@benchmark vec_contrastive_divergence_K(X_batch, rbm, 1, 0.01)\n",
    "@time vecopt_contrastive_divergence_K(X_batch, rbm, 1, 0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Char"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm_greys = PyPlot.cm_get_cmap(\"Greys_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PyPlot.imshow(reshape(rbm.W[9,:],28,28),cm_greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reshape(rbm.W[9,:],28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE arrays at the beginning of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expand(:(lr.*( Ehp * Xbatch' .-  Ehn *  Xneg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?A_mul_Bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expand(:(rbm.W * Xbatch .+ rbm.hid_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    Xneg = Xbatch\n",
    "    batch_size = size(Xbatch)[2]\n",
    "    \n",
    "    Hneg::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    Xneg::Array{Float64} = zeros(rbm.n_vis, batch_size)\n",
    "    \n",
    "    for k in 1:K\n",
    "        Hneg .= sigmoid.(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Xneg .= sigmoid.(rbm.W' * Hneg  .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "       \n",
    "    Ehp = sigmoid.(rbm.W * Xbatch .+ rbm.hid_bias)\n",
    "    Ehn = sigmoid.(rbm.W * Xneg .+ rbm.hid_bias)\n",
    "\n",
    "    Delta_W = lr.*( Ehp * Xbatch' .-  Ehn *  Xneg')\n",
    "    Delta_vis_bias = sum(lr .* (Xbatch .- Xneg), 2)[:]\n",
    "    Delta_hid_bias = sum(lr .* (Ehp .- Ehn), 2)[:]\n",
    "    \n",
    "    rbm.W .+= Delta_W ./ batch_size;\n",
    "    rbm.vis_bias .+= Delta_vis_bias ./ batch_size;\n",
    "    rbm.hid_bias .+= Delta_hid_bias ./ batch_size;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch = X_train[:,1:25]\n",
    "@benchmark vec_contrastive_divergence_K(X_batch, rbm, 3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vec_fit_CDK(X, rbm, batch_size::Integer,  n_epochs::Integer, K::Integer, lr::Real)\n",
    "        \n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    mb = 1\n",
    "    println(\"number minibatches:\", length(indicies), \"\\n\")\n",
    "    for epoch in 1:n_epochs\n",
    "        tic();\n",
    "        for minibatch_ind in indicies\n",
    "            Xbatch = @view X[:, minibatch_ind]\n",
    "            vec_contrastive_divergence_K(Xbatch, rbm, K, lr)\n",
    "        end\n",
    "        print(\"\\n\\nepoch \", epoch, \"  time epoch:\", toq(), \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 1000\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@time vec_fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define space for all the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    Xneg = copy(Xbatch)\n",
    "    batch_size = size(Xbatch)[2]\n",
    "    \n",
    "    local Hneg::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Xneg::Array{Float64} = zeros(rbm.n_vis, batch_size)\n",
    "    local Ehp::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Ehn::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    \n",
    "    for k in 1:K\n",
    "        Hneg .= sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Xneg .= sigmoid(rbm.W' * Hneg  .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "       \n",
    "    Ehp .= sigmoid(rbm.W * Xbatch .+ rbm.hid_bias)\n",
    "    Ehn .= sigmoid(rbm.W * Xneg .+ rbm.hid_bias)\n",
    "    \n",
    "    rbm.W .+= lr.*( Ehp * Xbatch' .-  Ehn *  Xneg') ./ batch_size;\n",
    "    rbm.vis_bias .+= sum(lr .* (Xbatch .- Xneg), 2)[:]./ batch_size;\n",
    "    rbm.hid_bias .+= sum(lr .* (Ehp .- Ehn), 2)[:] ./ batch_size;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch = X_train[:,1:25]\n",
    "@benchmark vec_contrastive_divergence_K(X_batch, rbm, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vec_fit_CDK(X, rbm, batch_size::Integer,  n_epochs::Integer, K::Integer, lr::Real)\n",
    "        \n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    mb = 1\n",
    "    #println(\"number minibatches:\", length(indicies), \"\\n\")\n",
    "    for epoch in 1:n_epochs\n",
    "        tic();\n",
    "        for minibatch_ind in indicies\n",
    "            Xbatch = @view X[:, minibatch_ind]\n",
    "            vec_contrastive_divergence_K(Xbatch, rbm, K, lr)\n",
    "        end\n",
    "        #print(\"\\n\\nepoch \", epoch, \"  time epoch:\", toq(), \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@benchmark vec_fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.gemm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.gemv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.ger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The blas has understandable names in Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?A_mul_Bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?At_mul_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?A_mul_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hneg= rand(rbm.n_hid, batch_size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(rbm.W' * Hneg)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BLAS.gemm('T','N', Float64(1.0), rbm.W, Hneg)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "At_mul_B(rbm.W, Hneg)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expand(:(rbm.W' * Hneg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@benchmark Ac_mul_B(rbm.W,Hneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@benchmark At_mul_B(rbm.W,Hneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    Xneg = copy(Xbatch)\n",
    "    batch_size = size(Xbatch)[2]\n",
    "    \n",
    "    local Hneg::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Xneg::Array{Float64} = zeros(rbm.n_vis, batch_size)\n",
    "    local Ehp::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Ehn::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    \n",
    "    for k in 1:K\n",
    "        #Hneg .= sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Hneg .= sigmoid.(A_mul_B(Rbm.W,Xneg) .+ rbm.hid_bias) .> rand()\n",
    "        #Xneg .= sigmoid(rbm.W' * Hneg  .+ rbm.vis_bias) .> rand()\n",
    "        Xneg .= sigmoid.(At_mul_B(rbm.W, Hneg) .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "       \n",
    "    Ehp .= sigmoid.( A_mul_B(rbm.W, Xbatch) .+ rbm.hid_bias)\n",
    "    Ehn .= sigmoid.( A_mul_B(rbm.W, Xneg) .+ rbm.hid_bias)\n",
    "   \n",
    "    #rbm.W .+= lr*( Ehp * Xbatch' -  Ehn *  Xneg') ./ batch_size;\n",
    "    rbm.W .+= lr.*(A_mul_Bt(Ehp, Xbatch) .- A_mul_Bt(Ehn, Xneg)) ./ batch_size;\n",
    "    rbm.vis_bias .+= sum(lr .* (Xbatch .- Xneg), 2)[:]./ batch_size;\n",
    "    rbm.hid_bias .+= sum(lr .* (Ehp .- Ehn), 2)[:] ./ batch_size;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 500\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@benchmark vec_fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating memory inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function vec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real, Hneg, Xneg, Ehp,Ehn  )\n",
    "    \n",
    "    Xneg = Xbatch\n",
    "\n",
    "    for k in 1:K\n",
    "        #Hneg .= sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Hneg .= sigmoid( rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        #Xneg .= sigmoid(rbm.W' * Hneg  .+ rbm.vis_bias) .> rand()\n",
    "        Xneg .= sigmoid(At_mul_B(rbm.W, Hneg) .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "       \n",
    "    Ehp .= sigmoid(rbm.W * Xbatch .+ rbm.hid_bias)\n",
    "    Ehn .= sigmoid(rbm.W * Xneg .+ rbm.hid_bias)\n",
    "   \n",
    "    #rbm.W .+= lr*( Ehp * Xbatch' -  Ehn *  Xneg') ./ batch_size;\n",
    "    rbm.W .+= lr.*(A_mul_Bt(Ehp, Xbatch) .- A_mul_Bt(Ehn, Xneg)) ./ batch_size;\n",
    "    rbm.vis_bias .+= sum(lr .* (Xbatch .- Xneg), 2)[:]./ batch_size;\n",
    "    rbm.hid_bias .+= sum(lr .* (Ehp - Ehn), 2)[:] ./ batch_size;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function mem_vec_fit_CDK(X, rbm, batch_size::Integer,  n_epochs::Integer, K::Integer, lr::Real)\n",
    "        \n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    mb = 1\n",
    "    println(\"number minibatches:\", length(indicies), \"\\n\")\n",
    "    \n",
    "    batch_size = length(indicies[1] )\n",
    "    local Hneg::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Xneg::Array{Float64} = zeros(rbm.n_vis, batch_size)\n",
    "    local Ehp::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Ehn::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    \n",
    "    for epoch in 1:n_epochs\n",
    "        tic();\n",
    "        for minibatch_ind in indicies\n",
    "            Hneg .= zero(Hneg)\n",
    "            Xneg .= zero(Xneg)\n",
    "            Ehp .= zero(Ehp)\n",
    "            Ehn .= zero(Ehn)\n",
    "            \n",
    "            vec_contrastive_divergence_K(X[:, minibatch_ind], rbm, K, lr, Hneg, Xneg, Ehp,Ehn  )\n",
    "        end\n",
    "        print(\"\\n\\nepoch \", epoch, \"  time epoch:\", toq(), \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 1000\n",
    "K = 1\n",
    "lr = 0.01\n",
    "\n",
    "@benchmark mem_vec_fit_CDK(X_train, rbm, batch_size,  n_epochs, K, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Optimized vectorial\n",
    "\n",
    "Use BLAS directly to make the \"transposes\"\n",
    "\n",
    "- https://discourse.julialang.org/t/blas-performance-issues-for-common-neural-network-patterns/565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.gemv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.gemv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xbatch = X_train[:,1:25]\n",
    "Xneg = copy(Xbatch)\n",
    "Hneg = sigmoid(rbm.W * Xneg .+ rbm.hid_bias);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?BLAS.gemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(rbm.W' * Hneg)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BLAS.gemm('T','N', Float64(1.0), rbm.W, Hneg)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@benchmark rbm.W' * Hneg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@benchmark BLAS.gemm('T','N', Float64(1.0), rbm.W, Hneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[3,4,5] .> rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "function optvec_contrastive_divergence_K(Xbatch, rbm, K::Integer, lr::Real)\n",
    "    \n",
    "    Xneg = copy(Xbatch)\n",
    "    batch_size = size(Xbatch)[2]\n",
    "    \n",
    "    # I put the line below here because then \n",
    "    # Hneg = sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand() \n",
    "    # is cast as an Array{float64} and then I can use the BLAS \n",
    "    # without errors\n",
    "    local Hneg::Array{Float64} = zeros(rbm.n_hid, batch_size)\n",
    "    local Xneg::Array{Float64} = zeros(rbm.n_vis, batch_size)\n",
    "\n",
    "    for k in 1:K\n",
    "        Hneg .= sigmoid(rbm.W * Xneg .+ rbm.hid_bias) .> rand()\n",
    "        Xneg .= sigmoid(BLAS.gemm('T','N', Float64(1.0), rbm.W, Hneg)  .+ rbm.vis_bias) .> rand()\n",
    "    end\n",
    "       \n",
    "    Ehp = sigmoid(rbm.W * Xbatch .+ rbm.hid_bias)\n",
    "    Ehn = sigmoid(rbm.W * Xneg .+ rbm.hid_bias)\n",
    "\n",
    "    Delta_W = lr*( Ehp * Xbatch' -  Ehn *  Xneg')\n",
    "    Delta_vis_bias = sum(lr .* (Xbatch .- Xneg), 2)[:]\n",
    "    Delta_hid_bias = sum(lr .* (Ehp - Ehn), 2)[:]\n",
    "    \n",
    "    rbm.W .+= Delta_W ./ batch_size;\n",
    "    rbm.vis_bias .+= Delta_vis_bias ./ batch_size;\n",
    "    rbm.hid_bias .+= Delta_hid_bias ./ batch_size;\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch = X_train[:,1:25]\n",
    "@benchmark optvec_contrastive_divergence_K(X_batch, rbm, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function vec_fit_CDK(X, rbm, batch_size::Integer,  n_epochs::Integer, K::Integer, lr::Real)\n",
    "        \n",
    "    n_samples = size(X)[2]\n",
    "    indicies = [x:min(x + batch_size-1, n_samples) for x in 1:batch_size:n_samples]\n",
    "    mb = 1\n",
    "    println(\"number minibatches:\", length(indicies), \"\\n\")\n",
    "    for epoch in 1:n_epochs\n",
    "        tic();\n",
    "        for minibatch_ind in indicies\n",
    "            Xbatch = @view X[:, minibatch_ind]\n",
    "            vec_contrastive_divergence_K(Xbatch, rbm, K, lr)\n",
    "        end\n",
    "        print(\"\\n\\nepoch \", epoch, \"  time epoch:\", toq(), \"\\n\")\n",
    "    end\n",
    "    rbm.trained = true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expand(:(Delta_W .+= lr * ( x * ehp' - xneg * ehn')'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?A_mul_Bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0-dev",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
