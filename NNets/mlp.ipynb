{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy NNets for education pourpouses in  Julia\n",
    "\n",
    "Interesting discussion how to make forward pass efficiently using BLAS:\n",
    "\n",
    "- https://discourse.julialang.org/t/blas-performance-issues-for-common-neural-network-patterns/565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[5.0,0.0,4.0,1.0,9.0,2.0,1.0,3.0,1.0,4.0  …  9.0,2.0,9.0,5.0,1.0,8.0,3.0,5.0,6.0,8.0])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = MNIST.traindata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train[1];\n",
    "y_train = train[2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       " 6.0\n",
       " 7.0\n",
       " 8.0\n",
       " 9.0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort(unique(train[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear layer and relu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "n_visible = 784\n",
    "n_hidden = 500\n",
    "\n",
    "srand(1234)\n",
    "W1 = rand(T, n_hidden, n_visible );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,784)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,10)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train[:,1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×10 Array{Float64,2}:\n",
       " 13027.4  14847.2  10032.1   8841.98  …  9508.38  17518.3  5716.2   11275.0 \n",
       " 14086.0  16402.7   9930.5   9091.55     8630.12  17810.7  5409.04  11826.5 \n",
       " 12056.6  15166.0   8827.64  8437.43     8358.53  16801.2  4946.36  10161.1 \n",
       " 14100.1  15136.6   9994.82  7975.77     9229.68  17994.4  5289.07  10823.4 \n",
       " 13865.6  15790.8   9572.64  8406.93     8977.36  18329.5  5107.01  10836.5 \n",
       " 13293.7  15751.5   9085.16  8706.94  …  8279.63  16631.5  5301.57   9686.37\n",
       " 13012.2  14699.1   8958.21  7850.34     8374.82  17603.9  5581.95   9880.36\n",
       " 12612.6  15642.7   9316.4   8956.66     8878.85  16938.1  5414.17  10578.8 \n",
       " 13888.1  15245.1   9318.4   8850.87     8042.99  18003.9  5010.2   10644.4 \n",
       " 12934.6  15676.7  10495.0   8267.86     8636.57  18339.1  5117.54  11485.0 \n",
       " 14082.3  15518.7   9735.68  9012.78  …  9074.54  18323.8  5688.01  11034.4 \n",
       " 14101.5  16376.5   9696.03  8447.71     9571.04  18369.6  6213.38  10135.2 \n",
       " 13291.9  15373.1  10304.7   8296.81     8532.4   17589.5  5311.18  10752.7 \n",
       "     ⋮                                ⋱                                     \n",
       " 14289.9  15992.4   9786.77  7980.77     8212.59  18297.5  5164.14  10962.8 \n",
       " 13470.3  15195.8   9732.85  8839.45     8712.13  17353.9  4847.58  10046.2 \n",
       " 13141.1  14717.5  10167.6   8690.17  …  8541.0   17157.2  5233.51  10749.6 \n",
       " 12720.3  15084.0  10284.6   8446.39     8213.76  17806.4  4574.48  10712.2 \n",
       " 14087.7  15585.9   9018.57  8021.12     8972.22  17770.3  5498.54  10566.7 \n",
       " 14366.0  16036.1  10337.3   8513.48     7977.77  18596.6  4735.14  10513.3 \n",
       " 14751.1  16810.9   9558.24  9366.49     9584.41  18801.3  5872.99  11998.4 \n",
       " 14242.8  16138.4   9630.09  8409.08  …  8183.6   17544.4  5282.88  10165.6 \n",
       " 14135.2  15388.1   9223.64  8223.58     8671.16  17831.4  5127.64  10092.3 \n",
       " 15143.8  15988.2   9500.8   7949.89     9548.82  18757.9  5402.2   10862.2 \n",
       " 13829.6  15994.2  11040.7   8520.76     8724.08  18553.1  5538.5   12520.5 \n",
       " 13655.7  14540.8   8761.05  9224.33     9146.01  17420.5  5136.4   10307.6 "
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 * X_train[:,1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workspace()\n",
    "type LinearLayer{T}\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    b::Vector{T}\n",
    "    seed::Int\n",
    "    \n",
    "    function LinearLayer(input, output; seed=1234)\n",
    "        srand(seed)\n",
    "        return new(input,output,rand(T,input,output)/sqrt(input), zeros(output))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "output_dim = 500\n",
    "l = LinearLayer{Float32}(input_dim,output_dim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workspace()\n",
    "type ReluActivation{T}\n",
    "    \"\"\"\n",
    "    Relu Activation function latyer\n",
    "    \"\"\"\n",
    "    dim::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax layer\n",
    "\n",
    "http://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workspace()\n",
    "type SoftMaxLayer{T}\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    seed::Int\n",
    "\n",
    "    function SoftMaxLayer(input, output; seed=1234)\n",
    "        srand(seed)\n",
    "        return new(input,output,rand(T,input,output)/sqrt(input))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "T = Float32\n",
    "W1 = rand(T, 500, 1000)\n",
    "W2 = rand(T, 500, 500)\n",
    "W3 = rand(T, 10, 500)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W3', error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W2', dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W1', dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN(input, error)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 2048, 512 * 512)\n",
    "W2 = rand(T, 1024, 2048)\n",
    "W3 = rand(T, 10, 1024)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN2(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W3, error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W2, dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W1, dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN2(input, error)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
